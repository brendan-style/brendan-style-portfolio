[
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Insightful Analytics",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPySpark Basics\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nBrendan Style\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYOUR NAME\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nStarwars\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYour Name\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nYOUR NAME\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/starwars/starwars_df.html",
    "href": "posts/starwars/starwars_df.html",
    "title": "Starwars",
    "section": "",
    "text": "Let’s analyze the starwars data:\nstarwars &lt;- read_csv(\"https://bcdanl.github.io/data/starwars.csv\")"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "href": "posts/starwars/starwars_df.html#variable-description-for-starwars-data.frame",
    "title": "Starwars",
    "section": "Variable Description for starwars data.frame",
    "text": "Variable Description for starwars data.frame\nThe following describes the variables in the starwars data.frame.\n\nfilms List of films the character appeared in\nname Name of the character\nspecies Name of species\nheight Height (cm)\nmass Weight (kg)\nhair_color, skin_color, eye_color Hair, skin, and eye colors\nbirth_year Year born (BBY = Before Battle of Yavin)\nsex The biological sex of the character, namely male, female, hermaphroditic, or none (as in the case for Droids).\ngender The gender role or gender identity of the character as determined by their personality or the way they were programmed (as in the case for Droids).\nhomeworld Name of homeworld"
  },
  {
    "objectID": "posts/starwars/starwars_df.html#human-vs.-droid",
    "href": "posts/starwars/starwars_df.html#human-vs.-droid",
    "title": "Starwars",
    "section": "Human vs. Droid",
    "text": "Human vs. Droid\n\nggplot(data = \n         starwars %&gt;% \n         filter(species %in% c(\"Human\", \"Droid\"))) +\n  geom_boxplot(aes(x = species, y = mass, \n                   fill = species),\n               show.legend = FALSE)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code with no space in the folder name.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brendan Style",
    "section": "",
    "text": "Brendan Style is a recent graduate from SUNY Geneseo, with a degree in Data Analytics. While at school, Brendan also spent his time competing as a member of the Geneseo Track and Field Team."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Brendan Style",
    "section": "Education",
    "text": "Education\nState University of New York at Geneseo | Geneseo, NY  B.S. in Data Analytics | Aug 2021 - May 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Brendan Style",
    "section": "Experience",
    "text": "Experience\nSUNY Geneseo | Data Analytics Tutor | August 2024 - Current"
  },
  {
    "objectID": "danl_proj_nba.html#salary-distribution-among-teams",
    "href": "danl_proj_nba.html#salary-distribution-among-teams",
    "title": "Data Analysis Project",
    "section": "Salary Distribution Among Teams",
    "text": "Salary Distribution Among Teams\nLet’s start with the salary distribution among teams using seaborn for visualization. ​​\n\n\n# Handle missing values in 'Salary' by replacing them with the median salary\nmedian_salary = nba['Salary'].median()\nnba['Salary'].fillna(median_salary, inplace=True)\n\n/var/folders/_m/d6jf0jhd2zzdfd5kzdhl_24w0000gn/T/ipykernel_79892/1671011424.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  nba['Salary'].fillna(median_salary, inplace=True)\n\n\n\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Calculate total salary by team\nteam_salary = (\n    nba\n    .groupby('Team')['Salary']\n    .sum()\n    .reset_index()\n    .sort_values(by='Salary', ascending=False)\n)\n\n# Plot total salary by team\nplt.figure(figsize=(10, 16))\nsns.barplot(data = team_salary,\n            x = 'Salary', y = 'Team',\n            palette = 'coolwarm')\nplt.title('Total Salary Distribution Among NBA Teams')\nplt.xlabel('Total Salary')\nplt.ylabel('Team')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThe visualization above displays the total salary distribution among NBA teams, with teams sorted by their total salary expenditure. This bar plot reveals which teams are the biggest spenders on player salaries and which are more conservative. The color gradient provides a visual cue to easily distinguish between the higher and lower spending teams.\nNotice that Portland Trail Blazers has the highest total salary followed by Golden State Warriors and Philadelphia 76ers, and Memphis Grizzlies has the lowest total salary."
  },
  {
    "objectID": "danl_proj_nba.html#player-age-distribution",
    "href": "danl_proj_nba.html#player-age-distribution",
    "title": "Data Analysis Project",
    "section": "Player Age Distribution",
    "text": "Player Age Distribution\nNext, let’s explore the Player Age Distribution across the NBA. We’ll create a histogram to visualize how player ages are distributed, which will help us understand if the league trends younger, older, or has a balanced age mix. ​​\n\n# Convert 'Birthday' column to datetime format\nfrom dateutil import parser\n# nba['Birthday'] = nba['Birthday'].apply(lambda x: parser.parse(x))\n\n# Now, let's calculate the age of each player\n# nba['Age'] = (datetime.now() - nba['Birthday']).dt.days // 365\n\n# Plot the age distribution of NBA players\nplt.figure(figsize=(10, 6))\nsns.histplot(nba['Age'],\n             bins = 15,\n             kde = True,\n             color = 'skyblue')\nplt.title('Age Distribution of NBA Players')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n\n/Users/bchoe/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThe histogram above shows the age distribution of NBA players, with a kernel density estimate (KDE) overlay to indicate the distribution shape. The plot helps identify the common ages for NBA players and whether there are significant numbers of very young or older players.\nNotice that the majority of players fall within an age range from 24 to 34. There are few players whose age is above 40."
  },
  {
    "objectID": "danl_proj_nba.html#position-wise-salary-insights",
    "href": "danl_proj_nba.html#position-wise-salary-insights",
    "title": "Data Analysis Project",
    "section": "Position-wise Salary Insights",
    "text": "Position-wise Salary Insights\nMoving on to Position-wise Salary Insights, we’ll examine how average salaries differ across player positions. This analysis could reveal which positions are typically higher-paid, potentially reflecting their value on the basketball court. Let’s create a box plot to visualize the salary distribution for each position. ​​\n\n# Plot salary distribution by player position\nplt.figure(figsize=(10, 6))\nsns.boxplot(data = nba,\n            x = 'Position', y = 'Salary',\n            palette = 'Set2')\nplt.title('Salary Distribution by Position')\nplt.xlabel('Position')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\nThe box plot above illustrates the salary distribution by player position, showcasing the variation in salaries among different positions within the NBA. PG-SG has the highest median salary."
  },
  {
    "objectID": "danl_proj_nba.html#top-10-highest-paid-players",
    "href": "danl_proj_nba.html#top-10-highest-paid-players",
    "title": "Data Analysis Project",
    "section": "Top 10 Highest Paid Players",
    "text": "Top 10 Highest Paid Players\nLastly, we’ll identify the Top 10 Highest Paid Players in the NBA. Let’s visualize this information.\n\n# Identify the top 10 highest paid players\ntop_10_salaries = nba.sort_values(by='Salary', ascending=False).head(10)\n\n# Plot the top 10 highest paid players\nplt.figure(figsize=(12, 8))\nsns.barplot(data = top_10_salaries,\n            x = 'Salary', y = 'PlayerName',\n            palette = 'viridis')\nplt.title('Top 10 Highest Paid NBA Players')\nplt.xlabel('Salary')\nplt.ylabel('Player')\nplt.show()\n\n\n\n\n\n\n\n\nThe bar plot above reveals the top 10 highest-paid NBA players, showcasing those who stand at the pinnacle of the league in terms of salary. This visualization not only highlights the star players who command the highest salaries but also may reflect their marketability, performance, and contribution to their respective teams."
  },
  {
    "objectID": "seaborn_basics.html",
    "href": "seaborn_basics.html",
    "title": "Seaborn Example",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Sample data\ndata = {\n    'Category': ['A', 'B', 'C', 'D'],\n    'Values': [23, 45, 56, 78]\n}\ndf = pd.DataFrame(data)\n\n# Create a barplot\nsns.set(style=\"whitegrid\")  # Optional: Set a clean grid style\nplt.figure(figsize=(8, 6))  # Set the figure size\nsns.barplot(data=df, x='Category', y='Values', palette='viridis')\n\n# Customize the plot\nplt.title(\"Bar Plot Example\", fontsize=16)\nplt.xlabel(\"Category\", fontsize=12)\nplt.ylabel(\"Values\", fontsize=12)\n\n# Show the plot\nplt.show()\n\nFutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(data=df, x='Category', y='Values', palette='viridis')"
  },
  {
    "objectID": "pandas_basics.html#creating-a-series",
    "href": "pandas_basics.html#creating-a-series",
    "title": "Pandas Basics",
    "section": "Creating a Series",
    "text": "Creating a Series\n\n\n# Creating a Series from a list\ndata = [10, 20, 30, 40, 50]\nseries = pd.Series(data)\nseries\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n10\n\n\n1\n20\n\n\n2\n30\n\n\n3\n40\n\n\n4\n50\n\n\n\n\ndtype: int64"
  },
  {
    "objectID": "pandas_basics.html#creating-a-dataframe",
    "href": "pandas_basics.html#creating-a-dataframe",
    "title": "Pandas Basics",
    "section": "Creating a DataFrame",
    "text": "Creating a DataFrame\n\n\n# Creating a DataFrame from a dictionary\ndata = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\ndf = pd.DataFrame(data)\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAlice\n25\nNew York\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#exploring-data",
    "href": "pandas_basics.html#exploring-data",
    "title": "Pandas Basics",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n\n# Display the first few rows\ndf.head()\n\n# Display the shape of the DataFrame\nprint(\"Shape:\", df.shape)\n\n# Display summary statistics\ndf.describe()\n\nShape: (3, 3)\n\n\n\n  \n    \n\n\n\n\n\n\nAge\n\n\n\n\ncount\n3.0\n\n\nmean\n30.0\n\n\nstd\n5.0\n\n\nmin\n25.0\n\n\n25%\n27.5\n\n\n50%\n30.0\n\n\n75%\n32.5\n\n\nmax\n35.0"
  },
  {
    "objectID": "pandas_basics.html#selecting-data",
    "href": "pandas_basics.html#selecting-data",
    "title": "Pandas Basics",
    "section": "Selecting Data",
    "text": "Selecting Data\n\n# Selecting a single column\ndf[\"Name\"]\n\n\n\n\n\n\n\n\nName\n\n\n\n\n0\nAlice\n\n\n1\nBob\n\n\n2\nCharlie\n\n\n\n\ndtype: object\n\n\n\n# Selecting multiple columns\ndf[[\"Name\", \"City\"]]\n\n\n  \n    \n\n\n\n\n\n\nName\nCity\n\n\n\n\n0\nAlice\nNew York\n\n\n1\nBob\nLos Angeles\n\n\n2\nCharlie\nChicago\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Selecting rows by index\ndf.iloc[0]\n\n\n\n\n\n\n\n\n0\n\n\n\n\nName\nAlice\n\n\nAge\n25\n\n\nCity\nNew York\n\n\n\n\ndtype: object"
  },
  {
    "objectID": "pandas_basics.html#filtering-data",
    "href": "pandas_basics.html#filtering-data",
    "title": "Pandas Basics",
    "section": "Filtering Data",
    "text": "Filtering Data\n\n# Filtering rows where Age is greater than 25\nfiltered_df = df[df[\"Age\"] &gt; 25]\nfiltered_df\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nBob\n30\nLos Angeles\n\n\n2\nCharlie\n35\nChicago"
  },
  {
    "objectID": "pandas_basics.html#adding-a-new-column",
    "href": "pandas_basics.html#adding-a-new-column",
    "title": "Pandas Basics",
    "section": "Adding a New Column",
    "text": "Adding a New Column\n\n\n# Adding a new column\ndf[\"Salary\"] = [50000, 60000, 70000]\ndf\n\n\n  \n    \n\n\n\n\n\n\nName\nAge\nCity\nSalary\n\n\n\n\n0\nAlice\n25\nNew York\n50000\n\n\n1\nBob\n30\nLos Angeles\n60000\n\n\n2\nCharlie\n35\nChicago\n70000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n    ## Conclusion\n\n    This notebook covers the basic operations of pandas. You can explore more advanced features like merging,\n    joining, and working with time series data in pandas documentation: https://pandas.pydata.org/docs/"
  },
  {
    "objectID": "posts/pyspark/posts/welcome/index.html",
    "href": "posts/pyspark/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/pyspark/index.html",
    "href": "posts/pyspark/index.html",
    "title": "pyspark",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 17, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pyspark/about.html",
    "href": "posts/pyspark/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/pyspark/posts/post-with-code/index.html",
    "href": "posts/pyspark/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/PySpark Basics/index.html",
    "href": "posts/PySpark Basics/index.html",
    "title": "PySpark Basics",
    "section": "",
    "text": "This is a blog post explaining some of the basics of Apache Hadoop, Apache Spark, and PySpark that we have learned so far in class.\n\nApache Hadoop\nApache Hadoop was created explicitly to help analysts deal with storing large datasets(usually exceeding 1gb). The primary way it does this is by breaking up the data into blocks and storing those blocks across different servers. This allows for both faster run times, by minimizing data movement through MapReduce, as well as minimizing errors, since, even if one of the blocks fails, the rest can still be altered successfully. The main drawback of Hadoop is that it cannot process events happening in real time, such the fluctuation of stock prices.\n\n\nApache Spark\nApache Spark deals more with the data analysis side of the datasets. It accomplishes these queries of large datasets by utilizing a structure of clusters that able to potentially take big applications and divide them into smaller tasks that take overall less computing power. Though this is a very powerful tool for creating queries, a shortcoming of Spark is a lack of a personal storage system, which is why many will use it in tandem with the storage capabilities of Hadoop, since Spark’s cluster-based method of applying queries was designed for Hadoop’s block storage.\n\n\nPySpark\nPySpark is a Python library that allows the user to run a number of Apache Spark functions on Python. The storage methods oh a pyspark dataframe are similar to what we discussed earlier, with different blocks of data being stored on multiple different machines, which allows for loading datasets and performing queries that may be beyond the capabilities of one device. It optimizes query speed by not actually performing the query until an action is called that makes it necessary, a process referred to as “lazy evaluation”. Another feature of pyspark is the ability to recovery the data if one of these blocks/nodes of data fails at any point."
  },
  {
    "objectID": "posts/welcome/index.html#experience",
    "href": "posts/welcome/index.html#experience",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "#\n\n#\n\nStarbucks Analytics | Data Analyst Intern | May 2024 - Aug 2024\n\nThis is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "great_day.html",
    "href": "great_day.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "great_day/post-with-code/index.html",
    "href": "great_day/post-with-code/index.html",
    "title": "What is GREAT Day?",
    "section": "",
    "text": "This will be a blog with a short write-up about what GREAT day is all about"
  },
  {
    "objectID": "projects/proj_1/index.html",
    "href": "projects/proj_1/index.html",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "",
    "text": "This will be a blog post recapping my findings from my project for Geneseo’s GREAT Day in 2024, as a part of my Data Visualization Final Project."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPrediciting Home Runs in MLB\n\n\n\n\n\n\n\n\nApr 21, 2025\n\n\nBrendan Style\n\n\n\n\n\n\n\n\n\n\n\n\nNFL Draft Trends Analyzed and Visualized\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\nBrendan Style\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/proj_1/index.html#goals",
    "href": "projects/proj_1/index.html#goals",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "Goals",
    "text": "Goals\nWhen beginning this project, I had 3 questions I wanted to answer:\n\nWhat impact does drafting have on overall team success?\nWhat Impact does prior drafting success have on future drafting success?\nHow has the shift in NFL playstyle impacted overall draft strategy?"
  },
  {
    "objectID": "projects/proj_1/index.html#dataset-acquisitioncleaning-and-variable-creation",
    "href": "projects/proj_1/index.html#dataset-acquisitioncleaning-and-variable-creation",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "Dataset Acquisition/Cleaning and Variable Creation",
    "text": "Dataset Acquisition/Cleaning and Variable Creation"
  },
  {
    "objectID": "projects/proj_1/index.html#dataset-acquisitioncleaning",
    "href": "projects/proj_1/index.html#dataset-acquisitioncleaning",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "Dataset Acquisition/Cleaning",
    "text": "Dataset Acquisition/Cleaning\nCreation of the dataset was quite simple. Thanks to the draft library from nfl-data-py, I was able to load all NFL draft selections from 1980 onward into a python pandas dataframe."
  },
  {
    "objectID": "projects/proj_1/index.html#dataset",
    "href": "projects/proj_1/index.html#dataset",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "Dataset",
    "text": "Dataset\n\nAcquisition\nCreation of the dataset was quite simple. Thanks to the draft library from nfl-data-py, I was able to load all NFL draft selections beginning in 1980 into a dataframe, which I have provided a snapshot of below.\n\n\n\n\n\nseason\nround\npick\npfr_player_name\nposition\nw_av\n\n\n\n\n1987\n1\n10\nRod Woodson\nDB\n142\n\n\n1997\n1\n13\nTony Gonzalez\nTE\n95\n\n\n1985\n1\n16\nJerry Rice\nWR\n160\n\n\n1996\n1\n26\nRay Lewis\nLB\n160\n\n\n1985\n1\n1\nBruce Smith\nDE\n152\n\n\n1989\n1\n5\nDeion Sanders\nDB\n127\n\n\n2013\n3\n63\nTravis Kelce\nTE\n86\n\n\n1981\n1\n2\nLawrence Taylor\nLB\n146\n\n\n1981\n1\n8\nRonnie Lott\nDB\n123\n\n\n1994\n1\n2\nMarshall Faulk\nRB\n132\n\n\n1995\n1\n28\nDerrick Brooks\nLB\n142\n\n\n1983\n1\n9\nBruce Matthews\nG\n138\n\n\n1990\n1\n17\nEmmitt Smith\nRB\n129\n\n\n2001\n1\n5\nLaDainian Tomlinson\nRB\n129\n\n\n1990\n7\n192\nShannon Sharpe\nTE\n82\n\n\n1999\n1\n7\nChamp Bailey\nDB\n117\n\n\n2003\n3\n69\nJason Witten\nTE\n80\n\n\n1998\n1\n4\nCharles Woodson\nDB\n115\n\n\n1990\n1\n5\nJunior Seau\nLB\n133\n\n\n2010\n2\n42\nRob Gronkowski\nTE\n78\n\n\n\n\n\nAll told this dataset comes in at well over 10,000 observations, and provides much more data about the players than what is shown above. Things like career stats/accolades, college attended, etc.\n\n\nCleaning\nSince this research is attempting to overall career value as the main metric, it would be unfair to incorperate recent draft classes, since they haven’t had enough time to accrue a high enough overall career value. I picked the cutoff point as 2014, since, at the time, that meant that every player in the dataset had been drafted at least 10 seasons ago, which gave players enough time to rack up AV(approximate Value), which is Pro Football Reference’s take on WAR, or Wins Above Replacement, commonly seen in baseball. It is by no means perfect, but in a game like football where discerning a player’s lone contribution to the team is tricky to do, it is the best metric we have at our disposal. This left us with 9,647 players left to analyze.\nIt should also be noted that, for most of this analysis, I only included players who actually played a snap in the NFL. This removed almost 3000 more players from the dataset, leaving us with just over 6,500 obervations remaining.\n\n\nStandardizing wAV\nAs previously mentioned, I will be using PFR’s Weighted Approximate Value (wAV) metric to determine draft pick success. However, there are issues with this metric, and chief among them is the fact that not all positions accrue value at the same rate. For example, if a Center has an All-Pro caliber year, they will probably be worth somewhere around 16 AV. However, that number is routinely passed by 4-6 QB’s every year. Because of this, I decided to standardize every player’s wAV realative to their position. In doing this, we achieve what can be considered to be a more fair way of evaluating players across different positions. Here’s an example of how it works:\nPlayer X plays at a position where the average wAV for a player’s career was 5, and the standard deviation is 2. If player X’s wAV for his career is 9, then his standardized wAV would be 2.0, since his wAV is 4 higher than the average, which is 2 standard deviaitons.\nFor a real world example, Peyton Manning (QB) and Zach Thomas (LB) had a dispairty of over 60 wAV between their career totals(176 vs 115). Using just that stat, you would think that Manning was far and beyond the better player, but according to their position-weighted standardized values, they were of equal relative value, both being rated at 3.67 standard deviations above their positional average. This means that Manning was of similar value to QB’s as Thomas was to LB’s.\nNow, this still isn’t a perfect measurement of a player’s value. The main issue is that this metric tends to undervalue QB’s as opposed to overvaluing. This is because QB’s and other positions like TE that have a smaller pool of players to draw from, so outlier careers, such as Manning or Kelce, can significantly influence the overall mean for the position group. This explains why Rod Woodson, who was certainly one of the best DB’s to ever play, is rated as far and beyond the most valuable player in NFL history by this new metric; he was a great player who had a long career, and did so at a position that sees more players take snaps than any other, meaning the positional mean is very low. For the entire sample, there were 341 drafted QB’s who took a snap in the NFL, as opposed to 400 snap-taking DB’s… in the 80’s alone.\n\n\nDummy Variables\nNow that we have the position-agnostic swAV, or Standardized Weighted Approximate Value, we can create a set of dummy variables that can categorize a player’s career. It is worth noting that, as the graph below indicates, there is a serious skew towards numbers between 0 and -1.\n\nBecause of this, if we want to create these dummy variables, we will have to do so based on our perception of average roster construction, which isn’t an exact science. Nonetheless, here are the groupings we came up with:\n\nBad: less than -0.70\nBelow Average: -0.70 to -0.20\nAverage: -0.20 to 0.70\nAbove Average: 0.70 to 1.50\nGood: 1.5 to 2.5\nGreat: over 2.5\n\nThese partitions created this distribution, which we believe to be a fair assessment of the average NFL roster:\n\nNow that we have our new variables, we can finally dive into the analysis."
  },
  {
    "objectID": "projects/proj_1/index.html#impact-of-draft-success-on-winning",
    "href": "projects/proj_1/index.html#impact-of-draft-success-on-winning",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "Impact of Draft Success on Winning",
    "text": "Impact of Draft Success on Winning\nIn order to measure the correlation of draft success on team success, I took a team’s total winning percentage for a given decade(1980’s up to 2010’s) and compared it to that team’s average value for a draft pick over that same time, and these were the results:\n\nAs you can see, there certainly is a correlation there, but not quite as high as initially expected, with a moderate Pearson correlation coefficient of just 0.30. This makes some sense when considering that players are not solely acquired from the draft, NFL organizations can also acquire players through free agency or via trading, but still a lower positive relation than what was expected.\nAfter seeing these results, I thought that my initial process might have been unfairly punishing teams that accumulated more late round picks, a strategy that is overall good for team building, but greatly reduces the team’s average draft pick value. In order to get around this, I decided to run a similar test to the last, but this time only use a team’s top 5 picks from each draft, so that a team wouldn’t be punished for having more selections. Here were the results of that comparison:\n\nUltimately, there wasn’t much of a change in correlation with this comparison, with this graph having a Pearson R-squared of 0.36, a marginal upgrade over the previous. Despite the draft being the easiest and most common venue by which NFL team’s aquire players, there is nothing but a moderate correlation between their drafting success and on the field success."
  },
  {
    "objectID": "projects/proj_1/index.html#more-impactful-late-round-success-or-early-round-failure",
    "href": "projects/proj_1/index.html#more-impactful-late-round-success-or-early-round-failure",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "More Impactful: Late Round Success or Early Round Failure?",
    "text": "More Impactful: Late Round Success or Early Round Failure?\nWhile researching the success of the draft on overall success, I came across an interesting discovery: The correlation between rounds 4-12 success and winning percentage was actually higher than the correlation between rounds 1-3 drafting success and winning percentage. This was surprising for a number of reasons; Not only do high round picks often become much more impactful players than later round picks(32.9% of all players drafted 1-3 become at least Above Average, as oppose to just under 10% for players drafted 4-12), but most teams also take extra care to select the most talented player available in the early rounds, whereas later rounds often see teams “reach” for players that fill a position of need. In order to investigate this further, I decided to create a pair of new dummy variables:\n\nSteal\nA “steal” is defined as a player whose value exceeds that of their expectations as defined by their draft position. To quantify it, we have used our dummy variables from earlier to come up with this definition:\n\n“Great” in round 2 or later\n“Good” in round 3 or later\n“Above Average” in round 4 or later\n“Average” in round 5 or later\n\n\n\nWhiff\nA “whiff” is the antithesis of a steal, being a player whose value falls short of their expectations, specifically:\n\n“Average” in round 1\n“Below Average” in round 2\n“Bad” in round 3\n\nWith these new variables, we can compare the occurrence of these variables with the teams winning percentage to see which has a larger impact on a team’s success.\n\n\nFirst, we have impact of Whiffs on success:\n\n\n\nThen we have Steals on success:\n\nAs you can see from the whiffs graph, there is virtually no correlation between missing on early-round picks and games won. The r-squared value, which would be expected to have a negative relationship, actually shows a slight positive relationship of 0.07, so we can confidently say that missing on early round picks has little to no impact on success. While the steals graph may not look that much better, there is a far higher correlation of 0.41.\nThis test firmly states that finding solid but not spectacular players in later rounds helps winning more so than drafting mediocre or bad players with valuable early round picks hurts it. These results reinforce the idea that many football analysts have come around to in recent years: The notion that trading higher value picks away for more chance to select players with less valuable picks, as more selections gives you a greater chance to find a “diamond in the rough”, so to speak. In a world where many believe that predicting which college players will be good NFL players is far harder than most give it credit for, quantity of picks can be just as, if not more valuable than quality of picks."
  },
  {
    "objectID": "projects/proj_1/index.html#impact-of-current-draft-success-on-future-draft-success",
    "href": "projects/proj_1/index.html#impact-of-current-draft-success-on-future-draft-success",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "Impact of Current Draft Success on Future Draft Success",
    "text": "Impact of Current Draft Success on Future Draft Success\nDespite every team having equal resources and access to film and scouting on college players, there is a prevailing theory among fans that certain teams(such as the Eagles in recent years) have an edge over every other organization when it comes to the draft, always pick up players for less than they’re worth, and always maintain success as a cause of it. While this certainly can be true in smaller sample sizes, most teams over longer periods of time will regress to the mean. Over this sample size, there are only 3 teams that don’t have an average pick value between 0.1 and -0.1: The Ravens and Browns, who rank first and last respectively, and the Steelers, who rank second highest. Both the Browns and Ravens both have fewer total picks than most teams in the sample, since they were both expansion franchises, and the Steelers are largely being carried by drafting the aforementioned Rod Woodson, who breaks the swAV scale with a massive 5.75. The other 29 NFL organizations are all within 0.1 to -0.08 swAV on their average pick… on a scale from -2 to 5.75. For reference, here’s what the difference looks like on a scale from -1 to 1:\n\nBecause of this, I was expecting to find essentially no correlation between draft success in one 5 year period and success in the next 5 year period. However, I was surprised to actually find a slight negative correlation of -0.11. This seems strange at first, until you remember that a good draft is at least partially expected to have a positive impact on the team’s winning percentage, as we proved earlier, and that increased winning percentage leads directly to lower-value draft picks. The small absolute value of the r-squared value indicates that the draft is still random for the most part, but the slight lean towards negative can be explained by the way the draft order is determined."
  },
  {
    "objectID": "projects/proj_1/index.html#changes-in-draft-strategy",
    "href": "projects/proj_1/index.html#changes-in-draft-strategy",
    "title": "NFL Draft Trends Analyzed and Visualized",
    "section": "Changes in Draft Strategy",
    "text": "Changes in Draft Strategy\nFinally, I want to cover changes in the overall gameplan of NFL offenses and defenses, and how that has been reflected in the draft.\n\nRB vs. WR\nFor most of the history of the NFL, the running back, the catalyst of the run game, was considered the most important part of the offenses, since only the most experimental of offenses threw the ball even 40% of the time. On the other hand, wide receivers, despite taking up more spots on the field, were seen as secondary pieces. That relationship continued that way mostly unchanged until a shift in offensive strategy took place sometime in the 80’s and really got going around the mid 90’s. Suddenly, wide receivers were seen as critical pieces of a team’s offense, and running backs were greatly diminished in value.\n\n\nAs a result, running backs, who were drafted significantly more than wide receivers from 1980-94, suddenly fell out of favor, and saw a steep 37% decline in draft frequency, whereas wide receivers were drafted 15% more often in the last 20 years compared to the previous 15\n\n\nLB vs. DB\nEssentially the WR vs. RB of defense, linebackers were seen as crucial to a team’s gameplan of stopping the run, while defensive backs were needed to halt the opposing wide receivers. Once again, linebackers were seen as more valuable due to the presence of the run game being more prevalent, and once again, the shift in strategy in the mid 90’s forced the NFL to change how they valued these players.\nNow, you will notice that, even in the first sample, defensive backs were still drafted more than linebackers, but that is because the blanket term of “defensive back” includes both corners and safeties, which combined take up 4-5 positions on the defense at any given time.\n\n\nAs the potency of opposing passing attacks increased, the NFL was forced to go after more and more DB’s in an effort to keep up, and linebackers were mostly left behind. Whereas DB’s saw an impressive 17% jump in draft frequency(after already being the most drafted position), LB’s saw their frequency fall by 22%.\n\n\nThat is all I have to share on this topic. This was a fun way to test my skills on Data Viz applications like Power BI, and I hope you enjoyed reading."
  },
  {
    "objectID": "projects/320_final/index.html",
    "href": "projects/320_final/index.html",
    "title": "Prediciting Home Runs in MLB",
    "section": "",
    "text": "Though baseball is a team sport, it really comes down to a series of 1-on-1 matchups between the pitcher and batter. Because of this, it becomes easy to isolate all other variables and just focus on the relationship between those two players, making baseball most likely the easiest sport to analyze and predict performance level. There are many projection systems out there that can usually be relied upon for an accurate prediction on a player’s full season value, but there are far fewer that will produce game-by-game projections, simply due to the smaller sample size leading to lower predictability. The type of projection that has the least professional documentation online is, without a doubt, the single game home run projection model. There are a few reasons for this; As was just mentioned, single game projections are very unpredictable, but also, because home runs are one of the rarest outcomes a batter-pitcher matchup(also referred to as a plate appearance) can produce. Throughout the course of any given game, there will usually be around 75 total plate appearances, and the majority of games only produce around 2-3, meaning that, for any single plate appearance, the odds of hitting a home run are around 3%. Since both teams have a finite number of hitters, you could increase those odds by taking the 3 plate appearances a player gets in one game and saying they will hit a home run in one of them, but even then the odds are no more than ~12% at best.\nThe attempts that have been made to predict these home runs with greater accuracy by incorporating previous data for both the batter and the pitcher often fall short by using too general of measurements. For this model, the key differentiator will be the use of these statistics on a per-pitch basis. This extra level of depth will help us assess each matchup based on the unique skills and tendencies of both the batter and pitcher, and, hopefully, will result in a greater accuracy than using aggregate stats based on a player’s entire body of work. In undertaking this task, we will be using a variety of statistical models to assess the accuracy and performance of the model. Namely, we will be utilizing a lasso linear regression model, a logistic regression model, and a gradient boosting algorithm. The main goal of this project is to determine if using pitch-level analysis makes predicting home runs significantly more accurate than using overall, aggregate statistics."
  },
  {
    "objectID": "projects/320_final/index.html#introduction",
    "href": "projects/320_final/index.html#introduction",
    "title": "Prediciting Home Runs in MLB",
    "section": "",
    "text": "Though baseball is a team sport, it really comes down to a series of 1-on-1 matchups between the pitcher and batter. Because of this, it becomes easy to isolate all other variables and just focus on the relationship between those two players, making baseball most likely the easiest sport to analyze and predict performance level. There are many projection systems out there that can usually be relied upon for an accurate prediction on a player’s full season value, but there are far fewer that will produce game-by-game projections, simply due to the smaller sample size leading to lower predictability. The type of projection that has the least professional documentation online is, without a doubt, the single game home run projection model. There are a few reasons for this; As was just mentioned, single game projections are very unpredictable, but also, because home runs are one of the rarest outcomes a batter-pitcher matchup(also referred to as a plate appearance) can produce. Throughout the course of any given game, there will usually be around 75 total plate appearances, and the majority of games only produce around 2-3, meaning that, for any single plate appearance, the odds of hitting a home run are around 3%. Since both teams have a finite number of hitters, you could increase those odds by taking the 3 plate appearances a player gets in one game and saying they will hit a home run in one of them, but even then the odds are no more than ~12% at best.\nThe attempts that have been made to predict these home runs with greater accuracy by incorporating previous data for both the batter and the pitcher often fall short by using too general of measurements. For this model, the key differentiator will be the use of these statistics on a per-pitch basis. This extra level of depth will help us assess each matchup based on the unique skills and tendencies of both the batter and pitcher, and, hopefully, will result in a greater accuracy than using aggregate stats based on a player’s entire body of work. In undertaking this task, we will be using a variety of statistical models to assess the accuracy and performance of the model. Namely, we will be utilizing a lasso linear regression model, a logistic regression model, and a gradient boosting algorithm. The main goal of this project is to determine if using pitch-level analysis makes predicting home runs significantly more accurate than using overall, aggregate statistics."
  },
  {
    "objectID": "projects/320_final/index.html#data",
    "href": "projects/320_final/index.html#data",
    "title": "Prediciting Home Runs in MLB",
    "section": "Data",
    "text": "Data\nTo acquire data for this project, I utilized the pybaseball library available on Python. I began my dataset in the year 2015, the beginning of the so-called “Statcast Era”, because many stats I would like to use are not available until then. I then collected every pitch either seen (in the hitter’s case) or thrown (in the pitcher’s case) so long both of these criteria were met:\n\nThe player threw/saw at least 1,000 pitches total in that season\nThe player saw/threw that individual pitch type at least 50 times.\n\nThe season-total minimum is instituted to ensure that all players in the sample saw/threw multiple different pitch types, whereas the singular-pitch minimum is in place so that there won’t be too many small samples with unrealistic results. All told, even with these qualifications, the sample size comes to 4.3 million individual pitches for the pitchers, and 4.8 million for the hitters.There we also 11 different pitch types tot analyze for pitchers and 9 for hitters, which I can cover briefly right now:\n\nFF: fastball\nSI: sinker\nSL: slider\nKC: knuckle-curve\nFS: splitter\nST: sweeper\nCH: changeup\nCU: curveball\nKN: knuckleball\nFO: forkball\n\nThese last 2 won’t show up in the hitters analysis due to too few pitchers throwing them. From this step, we needed these pitches aggregated on every player, every year they pitched, and every pitch type they threw. With that aggregation complete, we now have just over 13,000 observations for the hitters, and just over 9,000 for the pitchers. Here is a snapshot of the data:\n (Note: I will define the variables thoroughly in the Machine Learning Model section)\nAs you can see, the variables are currently in totals after aggregation. This can prove to be an issue because not every pitch was seen equally, which is shown by the barplot below:\n\nTo remedy this, we will put all stats on a rate basis, so the count of the pitches becomes irrelevant.\nHowever, our dataset is not complete just yet. Though we have rates, we haven’t yet adjusted for the offensive environment just yet. Over this 10 year dataset, the definition of “average” changes a significant margin in almost every variable we use. This is best illustrated in the case of fastball velocity. In 2015, the average fastball velo was just above 92mph, and in 2024, it has increased to more than 94mph. To someone who is unfamiliar with the way baseball is played, this may seem rather insignificant, but a 2mph difference can have a large impact. To illustrate this point, here is a graph of the average wOBA against all fastball velocities. Note that wOBA, or weighted on-base average, is an all-encompassing stat that aims to accurately measure a player or team’s overall offensive contributions to run production.\n\nThe difference between the 92 and 94 mile per hour brackets is roughly 20 points of wOBA production, which is more than significant enough to warrant consideration in this dataset. To solve this shifting-averages dilemma, we will take the average of every rate-based stat for each 2-year period, starting with 2015-16 and working up to 2023-24, for a total of 5 different “buckets” for stats to fall into. We will then compare the average stat for that pitch in that time frame to the stat of a particular player, and divide the player’s stat by that average to get a time period-normalized metric for each variable. So, for example, if a player from 2017 has a “whiff” rate of 30% on fastballs, and the average on fastballs in the 2017-2018 timeframe was 20%, that player’s “new” value would be a 1.50, or 50% higher than the league average. With this in place, it becomes far easier to compare players across different offensive environments. Here is an example of what our “final” dataset looks like after morphing:\n\nWith our dataset transformations complete, it is time to move on to the machine learning portion."
  },
  {
    "objectID": "projects/320_final/index.html#machine-learning-model",
    "href": "projects/320_final/index.html#machine-learning-model",
    "title": "Prediciting Home Runs in MLB",
    "section": "Machine Learning Model",
    "text": "Machine Learning Model\nFor this model, we will be utilizing classification models to test the capabilities of our dataset. Since we are attempting to predict home runs in individual matchups, we will be using real life webscrapped matchup data with our variables, as well as whether or not the matchup resulted in a home run. Over the past 10 years, we were able to scrape over 109k matchups that involve both a hitter and pitcher we have data on. Note that, since we want to see if this model has any predictive power, we are using year n data to predict year n+1 matchups, so 2015 matchups could not be included for this. Before we continue any further, let’s define all the variable we will be using:\n\nUniversal Variables\n\nxwOBAcon(expected weighted on-base average on contact): This stat should be the most correlated to homeruns. Essentially, what xwOBAcon attempts to measure is the expected production of a hitter based on their launch angle and exit velocity on batted balls. I chose a stat like this over something like expected Slugging Percentage because I felt that, in the long run, xwOBAcon does a better job of telling who the better hitters are.\nBarrel Rate: A “barrel” is a batted ball that meets a certain threshold of exit velocity-launch angle combination that typically produces the most desirable outcomes\nExit Velocity (EV): The average of all the exit velocities that pitch type registered\nFly Ball (FB): The percentage of batted balls on that pitch that resulted in a fly ball, which is a batted ball that achieves a launch angle between 20-50 degrees\nWeakly hit: Essentially the antithesis of barrel rate, a ball that is hit with a specific exit velo-launch angle combo that results in the least desirable outcomes\nHard-Hit (HH): A batted ball with a, exit velocity registered at 95mph or greater\nWhiff: The percentage of swings where the ball was missed entirely\nLaunch Angle (LA):The average of all the launch angles that pitch type registered\nGround Ball (GB): The percentage of batted balls on that pitch that resulted in a ground ball, which is a batted ball that achieves a launch angle under 10 degrees\nLine Drive (LD): The percentage of batted balls on that pitch that resulted in a line drive, which is a batted ball that achieves a launch angle between 10-25 degrees\n\n\n\nPitcher-Only\n\nZ-move/X-move: The amount of vertical/horizontal movement the pitch experiences while traveling from the mound to the home plate, respectively.\nSpin Rate: The average amount of RPMs a pitch generates\nVelocity: The average velocity a pitch is thrown with\nExtension: How far the pitcher can extend his front leg off the mound when delivering the pitch\n\nWith our variables defined, we now need to run our models. As previously stated, we want to use classification models for the outcome prediction, so we will be utilizing XGBoost gradient boosting, as well as logistic regression.\nOne last note before we discuss the model performances; In an effort to keep the variable count from ballooning out of control, we decided to use a weighted average of both the hitter and pitcher’s performances against all pitch types, so we will only end up with one number for each variable. Here’s a SIMPLE example of how that works:\n\nBatter has a “barrel” rate of 1.8 against fastballs and a rating of 2.0 against curveballs\nPitcher has a barrel rate of 1.4 when throwing fastballs, and a rating of 1.0 for curveballs\n\nAdditionally, Pitcher throws his fastball 40% of the time, and the curve 60%\n\nFirst, we take the averages of the barrel rates\n\nSo the overall fastball barrel rate will be 1.6, and 1.5 for curveballs\n\nNext, we use a weighted formula based on how often they are thrown\n\nSo 1.6(0.4) + 1.5(0.6) = 1.54\n\n1.54 is the overall barrel rating for the matchup\n\nFor the pitcher only stats, we used the same weighted-average idea, just without involvement from the hitter data. Now it is finally time to discuss model performance.\n\n\nClassification Model and Threshold Issue\nUpon running both our logistic regression model, as well as our gradient boosting algorithm, we achieved accuracy scores of over 90% on both. However, a quick inspection into the results will reveal that every prediction, all 109k, we projected as a 0, meaning neither model predicted a single homerun. A closer look into the logistic regression model in particular reveals that the highest probability of a home run assigned to a matchup was just over 37%. Now, since home runs are a very rare occurrence, this all-zeros approach netted a very high accuracy, but this is far from a satisfying answer.\nSince no matchups achieved a 50% probability, I assumed it must be a problem with the threshold, and figured I would look at a double density plot to see what would work better. After viewing the plot, I came to the conclusion that a threshold of 0.0915, or a 9.15% chance of occurring, would work best here.\n\nYou will notice that, on top of being an extremely low threshold, both 1’s and 0’s have extremely similar density plots. This is likely due to both a combination of home run prediction being a very complex problem, as well as potentially lower-quality data. Once rerunning the logistic regression model with the threshold, this led to a confusion matrix with the following distribution:\n\nThough it is nice to see the model actually predicting 1’s, the metrics on this model are fairly poor. The accuracy and recall both sit at 57%, which isn’t terrible when considering the difficulty associated with predicting home runs on matchups basis. However, the precision sits at a staggering 12%, and the overall enrichment is 1.29. Though these metrics aren’t outright bad, none of them show a great deal more success than just random guessing, which remained true when I calculated the AUC graph:\n\nNow that it has been firmly established that our classification models simply won’t work very effectively, I thought of one more method that might be able to identify matchups with a vastly increased probability of a home run, if outright prediction won’t work.\n\n\nLASSO Regression and Predicted HR Ratings\nWhen morphing the dataset, I made sure to include a variable called “HR”, which is expressed as the player’s home run percentage on batted balls. Now that the classification models have proven to not be the most effective tool for this project, I will be putting all the other variables through a LASSO regression model in order to predict the player’s home run variable, and then seeing how these predicted HR ratings compare to the real ones.\n\nRegression Weights, Standard Error, and Level of Significance\nAfter running our regression model, here are the results for both the batters and pitchers.\n\nBatters\n\nA few notes regarding these results:\n\nMost features that were deemed insignificant simply do not have that much effect on the model (hh,whiff,la)\nExit Velocity is the main exception to this, as it owns one of the largest coefficients of all features, but ev is likely deemed insignificant due to having significant multicollinearity with both xwOBAcon and barrel rate, as well as having the largest standard error by far.\n\nDespite these features lacking significance, the model as a whole is statistically significant, so we can leave them in the model\n\n\nPitchers\n\n\n\nThe Pitcher Analysis Problem\nWith hitters, the analysis is straightforward; The inputs that they can control are also the inputs that correspond to home run production. Things like average exit velocity, fly ball rate, and xwOBAcon are all well within the hitters control. However, this does not also apply to pitchers. For pitchers, there are stats that correspond to home runs, such as the aforementioned xwOBAcon can exit velocity, and there are stats that they can control, such as spin rate, pitch velocity, and pitch movement. The two categories are, essentially, mutually exclusive. The reason we include the pitcher-controlled stats is because we want these ratings to be predictive, and if we only include stats that pitchers have little control over, there will be zero predictive value to the model. So, even though stats like x_move, extension, and spin rate all have no significance, they must be included to ensure that the model retains some predictive value.\n\n\n\nModel Accuracy\nNow that we have run our regression model, we must test to see if the model is working as intended. We have a variety of ways to check the accuracy of our model, and we can begin with the r-squared correlation coefficients, broken down by hitters and pitchers:\n\nFor current year value’s, the model performs very well, with a value that is generally considered “strong” for both hitters and pitchers. However, we are attempting to see if this model has some predictive power as well, and for that the results are more mixed. The hitters have a moderate correlation, which, for trying to predict something as rare as home run rate, is perfectly adequate, arguably even good given the circumstances. The pitcher’s however, remain extremely difficult to predict for the future, and this is something that exists for all of baseball analysts. Even the most in-depth pitcher stats you can find usually can’t manage more than a 0.2-0.3 R-squared value year over year.\nAnother, similar method of measuring the accuracy of our model would be to use a modified confusion matrix with redefined parameters. Since our outcome is a continuous variable, we can change the definition of a 1 or 0 to occur if the predicted variable is within a certain distance of the actual. For this, I ran 2 tests: One to see if the predicted value was within 30% of the actual, and one to see if the predicted value was within a range of 0.5 in either direction (on a scale that ranges from 0 to 8). For the 30% range, I achieved an accuracy of 52.4%. While this doesn’t sound very impressive, bear in mind that the vast majority of actual HR values are lower than 1, and almost all are lower than 2, so to be within 30% means likely being within 0.3 or less, which is a perfectly acceptable margin of error given the overall range of the values.\n\nOur other method of testing accuracy, being within a 0.5 range, resulted in 71% correctly assessed values, so it is safe to say that both this and the R-Squared values show our model is working as intended. However, since we want this model to be predictive, we have one more method of confirming it works.\n\n\nCalculating Matcup-Specific Ratings\nIn order to test whether or not these ratings will be adept at predicting future outcomes, we will use the weighted-average formula from earlier with the predicted home run ratings, which will create an overall rating. The probability of a home run being hit should, in theory, increase as this rating increases. We will break these ratings down into buckets, and measure the success rate of each bucket by taking the number of home runs hit divided by the number of matchups in that bucket. Since we are also testing whether or not using pitch-by-pitch stats produces better results than using aggregate statistics, we will have multiple control tests as well.\nFor the first control, we have ratings that were calculated using our lasso-predicted home run ratings, but used aggregate statistics instead of pitch-level:\n\nThough there appears to be an upward trend for most of the graph, the steep decline for the highest ratings isn’t very confidence inspiring, and the fact that the percentage of home runs is lower in the 4+ bracket than the 2-3 bracket indicates that these ratings may not be very accurate.\nNext, we have the ratings created by using pitch-level data, but with the actual prior-year HR ratings, instead of the lasso-predicted ones:\n\nThe shape of this graph leads us to believe that the pitch-level data is more accurate at predicting future HR outcomes, as the top group is essentially tied with the second highest bracket, instead of being far worse. However, this still isn’t as conclusive as we would like. Just under a 16% chance of a home run is nice, but we are looking for better results out of our top group.\nLastly, we have the ratings as calculated using the pitch-level data AND our lasso-predicted ratings, which we believe should yield the most accurate results:\n\nAs can be discerned from the graph, this method produced the most desirable results. For the most part the results are the same as the prior graph, but the top bracket has now produced a rating of just over 23%. When you consider that the usual chance for a batter-pitcher matchup with multiple plate appearances to result in a home run is right around 10%, using these ratings to find a grouping where the odds are well north of 20% is phenomenal. Though this isn’t a traditional method of measuring success, the fact that this third method performed noticeably better than the first two indicates to us that the lasso regression model worked as intended, and that pitch-level data does result in more accurate predictions."
  },
  {
    "objectID": "projects/320_final/index.html#real-world-applicationsconclusion",
    "href": "projects/320_final/index.html#real-world-applicationsconclusion",
    "title": "Prediciting Home Runs in MLB",
    "section": "Real-World Applications/Conclusion",
    "text": "Real-World Applications/Conclusion\nUnfortunately, since this project deals solely with the world of sports, it is quite limited in terms of real world application. I truthfully don’t like to condone this in a paper that is aiming to be professional, but the only context in which I think you could use this information would be the realm of sports betting. Since the LASSO regression model works by loosely calculating a probability of a matchup resulting in a home run, those probabilities can be used to calculate a perceived “value” over a given sports book. For a quick example:\n\nPlayer A is a hitter facing player B, a pitcher\nThe matchup rating for these players is 5.3, so we can assumes the odds of a home run being produced is a little over 20%\nThe stated odds of this matchup resulting in a homerun is +500, which we can easily calculate to being ⅙ or a 16.7% chance\nSince the stated odds are lower than the projected odds based on the lasso formula, we could now consider it a “value” selection, where betting on this player to hit a home run would theoretically result in a positive EV given enough simulations of the event.\n\nAlongside working on this project, I actually did attempt something like this, albeit with “theoretical” money. Every day of the 2025 baseball season, I selected the 5 players with the greatest disparity between my calculated probability and their stated probabilities and bet 5 theoretical dollars on them. Doing this for a little over 3 weeks, I was actually able to make a hypothetical profit of 37%, so even though it was a small sample size, it would appear that this method has some potential.\nOutside of betting, another thing that this could possibly be used for would be as a basis for a full game projection system. Since we are attempting to predict the outcomes of matchups, we could extend this past homeruns and attempt to predict the probability of all sorts of events. T Using that, you could run enough simulations of these matchups that you could predict the 50th percentile outcome of any given game. While that in itself might not have any non-betting applications, it would be very fun to try, and maybe something I tackle in the future.\nOverall this project taught me a lot about the applications of machine learning models. Specifically, it taught me the importance of high-quality data when it comes to classification models, and it also taught me how to think outside the box and use machine learning models in ways I hadn’t really thought of before, such as creating projected HR ratings with the use of LASSO. Though my conclusion wasn’t a very traditional method of success, I think it showed enough promise that I can consider this whole project a valid use of my time. To wrap things up, I would just like the reiterate some of the shortcomings I touched on in my presentation:\n\nThese ratings do not include modifiers for either handedness splits or stadium factors. For context, most hitters usually perform better when facing a pitcher of the opposite hand (so a lefty hitter will hit better vs a right pitcher), and some stadiums have dimensions that allow more home runs to be hit. Though both of these likely would’ve played a minimal role in the outcomes, it is certainly worth mentioning\nPerhaps the biggest shortcoming is the use of strictly prior-season data. Most high level projection systems and game simulation engines will use a combination of multiple seasons worth of data, including the current one. This is something that simply had to be cut for time constraints, but I certainly would factor it in if I continue to flesh out this project.\nAnother one of the largest shortcomings was the lack of ratings for any other pitcher besides the starters. These ratings only factor in pitchers that pitch around 60% of the game, called starters, but the pitchers that pitch afterwards, called relievers, were not. This would be tricky to do given the unpredictability of which reliever will pitch when, but it is still worth mentioning that these ratings aren’t emblematic of the entire game, which I originally set out to do.\nFinally, while working on this project, bat-tracking data was released for MLB, But due to the timing, and the fact that it only exists for the 2023 and 2024 seasons, I was unable to include it as part of the analysis. Still, as indicated by the graph below, it is most certainly impactful on home run hitting capabilities"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Home Run Betting Model",
    "section": "",
    "text": "This will be a publishing of my Home Run predictor model that I created for my Data Analytics Capstone during my senior year of college. The model will, if given information about a hitter’s matchup, produce a rating. These ratings have been reversed engineered to create predicted odds for hitting a home run. Using these, I will be publishing players every day that carry with them a combination of both good home run capability, as well as good percieved value.\n\nColumns\n\nPred_odds: Based on the model and data, their predicted betting odds for hitting a home run\nDiff: The difference between my predicted odds and the listed sportsbook odds, higher = more percieved value\nSportsbook: The sportsbook that features the highest (least likely) odds for the listed player\n\n\n\nSome things to note\n\nPlayers season long stats will be updated every Thursday\nHitters facing pitchers with insufficient data will be left off the list\nLineups/rosters are usually pulled early in the day, so some players published may not be in the starting lineups come game time\n\n\n\nComing Soon\n\nWeather Factors\nStadium factors based on Pulled-FB rate\n\nHere are the picks for today, we will be tracking the profit of these picks as the remainder of the season goes along. Enjoy!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplayer\npitcher\naway_team\nhome_team\npred_odds\ndiff\nbook\n\n\n\n\nRafael Devers\nYoshinobu Yamamoto\nLAD\nSF\n355\n245\nFanDuel\n\n\nNolan Jones\nAaron Civale\nCLE\nCWS\n408\n222\nFanDuel"
  },
  {
    "objectID": "model.html#coming-soon",
    "href": "model.html#coming-soon",
    "title": "Home Run Betting Model",
    "section": "Coming Soon",
    "text": "Coming Soon\n\nWeather Factors\nStadium factors based on Pulled-FB rate\n\nHere is the top 50 for ratings today, enjoy!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplayer\npitcher\naway_team\nhome_team\nrating\nFanDuel\nDraftKings\n\n\n\n\nAaron Judge\nChris Flexen\nCHC\nNYY\n9.21\n118\n155\n\n\nOneil Cruz\nJoe Ryan\nPIT\nMIN\n8.94\n340\n320\n\n\nCorey Seager\nLance McCullers\nTEX\nHOU\n8.87\n330\n390\n\n\nElly De La Cruz\nGerman Marquez\nCOL\nCIN\n8.74\n285\n265\n\n\nKetel Marte\nTyler Anderson\nARI\nLAA\n8.58\n240\n215\n\n\nNick Kurtz\nMax Scherzer\nTOR\nATH\n8.23\n285\n255\n\n\nJames Wood\nQuinn Priester\nWSH\nMIL\n8.18\n450\n450\n\n\nWill Benson\nGerman Marquez\nCOL\nCIN\n8.11\n310\n290\n\n\nEugenio Suarez\nTyler Anderson\nARI\nLAA\n8.08\n285\n255\n\n\nCorbin Carroll\nTyler Anderson\nARI\nLAA\n8.06\n300\n265\n\n\nShohei Ohtani\nLogan Webb\nLAD\nSF\n7.60\n390\n330\n\n\nJesus Sanchez\nDean Kremer\nMIA\nBAL\n7.58\n400\n370\n\n\nRandal Grichuk\nTyler Anderson\nARI\nLAA\n7.57\nNA\n265\n\n\nJuan Soto\nMichael Wacha\nNYM\nKC\n7.44\n340\n310\n\n\nKyle Stowers\nDean Kremer\nMIA\nBAL\n7.43\n340\n350\n\n\nSeiya Suzuki\nCarlos Rodon\nCHC\nNYY\n7.29\n420\n400\n\n\nBrent Rooker\nMax Scherzer\nTOR\nATH\n7.15\n300\n300\n\n\nHunter Goodman\nChase Burns\nCOL\nCIN\n7.10\n280\n280\n\n\nRyan McMahon\nChase Burns\nCOL\nCIN\n7.07\n450\n450\n\n\nJackson Holliday\nEdward Cabrera\nMIA\nBAL\n7.04\n500\n475\n\n\nMichael Toglia\nChase Burns\nCOL\nCIN\n7.01\n440\n360\n\n\nMickey Moniak\nChase Burns\nCOL\nCIN\n6.84\n350\n400\n\n\nManny Machado\nRanger Suarez\nPHI\nSD\n6.80\n370\n340\n\n\nGunnar Henderson\nEdward Cabrera\nMIA\nBAL\n6.79\n420\n360\n\n\nDaniel Schneemann\nShane Smith\nCLE\nCWS\n6.78\n520\n450\n\n\nMike Trout\nRyne Nelson\nARI\nLAA\n6.78\n220\n225\n\n\nJazz Chisholm\nChris Flexen\nCHC\nNYY\n6.76\n340\n320\n\n\nAdolis Garcia\nLance McCullers\nTEX\nHOU\n6.71\n440\n425\n\n\nDansby Swanson\nCarlos Rodon\nCHC\nNYY\n6.56\n440\n500\n\n\nJoey Loperfido\nLuis Severino\nTOR\nATH\n6.55\n630\n600\n\n\nCal Raleigh\nTarik Skubal\nSEA\nDET\n6.53\n390\n450\n\n\nColton Cowser\nEdward Cabrera\nMIA\nBAL\n6.52\n450\n400\n\n\nPete Alonso\nMichael Wacha\nNYM\nKC\n6.52\n310\n260\n\n\nKyle Tucker\nCarlos Rodon\nCHC\nNYY\n6.44\n340\n300\n\n\nLawrence Butler\nMax Scherzer\nTOR\nATH\n6.44\n340\n300\n\n\nChristian Walker\nJack Leiter\nTEX\nHOU\n6.43\n450\n320\n\n\nZach Neto\nRyne Nelson\nARI\nLAA\n6.43\n360\n350\n\n\nTaylor Ward\nRyne Nelson\nARI\nLAA\n6.43\n400\n370\n\n\nJake Burger\nLance McCullers\nTEX\nHOU\n6.38\n480\n400\n\n\nGeorge Springer\nLuis Severino\nTOR\nATH\n6.36\n360\n330\n\n\nRiley Greene\nLuis Castillo\nSEA\nDET\n6.35\n350\n370\n\n\nVladimir Guerrero\nLuis Severino\nTOR\nATH\n6.35\n350\n300\n\n\nPete Crow-Armstrong\nCarlos Rodon\nCHC\nNYY\n6.34\n420\n370\n\n\nTyler Soderstrom\nMax Scherzer\nTOR\nATH\n6.31\n400\n310\n\n\nMatt Wallner\nPaul Skenes\nPIT\nMIN\n6.28\n500\n450\n\n\nWyatt Langford\nLance McCullers\nTEX\nHOU\n6.26\n500\n500\n\n\nEvan Carter\nLance McCullers\nTEX\nHOU\n6.25\n750\n900\n\n\nLuis Robert\nLogan Allen\nCLE\nCWS\n6.22\n420\n450\n\n\nColt Keith\nLuis Castillo\nSEA\nDET\n6.22\n540\n400\n\n\nWilyer Abreu\nJoe Boyle\nTB\nBOS\n6.17\n450\n450"
  }
]